# bank_transaction_fraud_detation


ì´ ë¬¸ì„œëŠ” **IEEE-CIS Fraud Detection** í˜•íƒœì˜ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬, **XGBoost**ë¡œ ì‚¬ê¸° ê±°ë˜ ì—¬ë¶€(`isFraud`)ë¥¼ ì˜ˆì¸¡í•˜ëŠ” íŒŒì´í”„ë¼ì¸ì„ ê°„ë‹¨íˆ ë³´ì—¬ì¤ë‹ˆë‹¤. 


- **ì£¼ìš” íë¦„**  
  1) **Train / Test ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°**  
  2) **ì»¬ëŸ¼ëª… ì „ì²˜ë¦¬**  
  3) **Train ë³‘í•©, Test ë³‘í•©**  
  4) **íƒ€ê¹ƒ ë¶„ë¦¬ (`isFraud`)**  
  5) **ê²°ì¸¡ì¹˜ ì²˜ë¦¬, ë¼ë²¨ ì¸ì½”ë”©**  
  6) **Train / Validation ë¶„í• **  
  7) **XGBoost ëª¨ë¸ í•™ìŠµ + ê²€ì¦** (í˜¼ë™í–‰ë ¬, ì •ë°€ë„/ì¬í˜„ìœ¨)  
  8) **ì „ì²´ ë°ì´í„°ë¡œ ì¬í•™ìŠµ í›„ ì œì¶œ íŒŒì¼ ìƒì„±**  


ì•„ë˜ ë‚´ìš©ì€ `bank_transaction_fraud_detaction_v5.ipynb` ë¼ëŠ” ì˜ˆì‹œ ì½”ë“œ íŒŒì¼ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.

---

## íŒŒì¼ êµ¬ì¡°

```plaintext
.
â”œâ”€â”€ train_transaction.csv
â”œâ”€â”€ train_identity.csv
â”œâ”€â”€ test_transaction.csv
â”œâ”€â”€ test_identity.csv
â”œâ”€â”€ sample_submission.csv

```



![image](https://github.com/user-attachments/assets/85beee6e-3c0d-45a2-b360-e6d1a6051199)
![image](https://github.com/user-attachments/assets/12919f8e-ae0c-4f19-9eb5-271f96554f5f)
![image](https://github.com/user-attachments/assets/0c179545-5255-4bca-bc19-ed13ee2c2605)




## Feedback(1ë“± ìˆ˜ìƒìì˜ ê¸€ì„ ì½ê³  ê¹¨ë‹¬ì€ ì )

**ê±°ë˜(Fraud) ì˜ˆì¸¡ì´ ì•„ë‹ˆë¼ â€œí´ë¼ì´ì–¸íŠ¸(ì‹ ìš©ì¹´ë“œ) ì‚¬ê¸° ì—¬ë¶€â€**ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë¬¸ì œ


í•œ ë²ˆ ì‚¬ê¸°ê°€ ë°œìƒí•œ ì¹´ë“œ(credit card)ëŠ” ê·¸ ë’¤ë¡œ ëª¨ë“  ê±°ë˜(isFraud=1)ë¡œ ì²˜ë¦¬ëœë‹¤ê³  í•¨.


ì¦‰, ì‚¬ì‹¤ìƒ â€œí•´ë‹¹ ì¹´ë“œë¥¼ ê°€ì§„ ì‚¬ìš©ìâ€ê°€ ì‚¬ê¸°ì¸ì§€ ì•„ë‹Œì§€ íŒë‹¨í•˜ëŠ” ë¬¸ì œì— ê°€ê¹ë‹¤ëŠ” ê²ƒ.


ë°ì´í„°ê°€ ì‹œê³„ì—´(Time Series)ë¡œ ë³´ì´ì§€ë§Œ, ì‹¤ì œë¡  â€˜ìƒˆë¡œìš´ í´ë¼ì´ì–¸íŠ¸â€™ë¥¼ ì˜ˆì¸¡í•´ì•¼ í•˜ëŠ” ì„±ê²©


Public LBì™€ Private LB ì‚¬ì´ ì°¨ì´ê°€ í° ì´ìœ  ì¤‘ í•˜ë‚˜ê°€, Private ì…‹ì— ì²˜ìŒ ë“±ì¥í•˜ëŠ”(Trainì— ì—†ë˜) ê³ ê°ì´ ë§ê¸° ë•Œë¬¸(ì•½ 68.2%).


ì¦‰, Trainê³¼ Public Test ì¼ë¶€ëŠ” ê°™ì€ í´ë¼ì´ì–¸íŠ¸ê°€ ì¡´ì¬í•  ìˆ˜ ìˆì§€ë§Œ, Private Testì—ëŠ” ìƒˆë¡œìš´ í´ë¼ì´ì–¸íŠ¸ê°€ ëŒ€ê±° í¬í•¨.


**UID(Unique Identifier)**ë¥¼ í†µí•´ í´ë¼ì´ì–¸íŠ¸ë¥¼ ì‹ë³„í•˜ê³ , ê·¸ ê·¸ë£¹ ë‹¨ìœ„ë¡œ íŠ¹ì§•ì„ ì§‘ê³„(Aggregate)í•˜ëŠ” ë°©ì‹


ì˜ˆ: card1, addr1, ê·¸ë¦¬ê³  D1n(ê±°ë˜ ì‹œì‘ ë‚ ì§œ ê¸°ë°˜ íŒŒìƒ)ì„ í•©ì³ì„œ ì„ì˜ì˜ uidë¥¼ ë§Œë“  ë’¤, uidë³„ë¡œ ë‹¤ì–‘í•œ ì»¬ëŸ¼ì„ í‰ê· , í‘œì¤€í¸ì°¨, nunique ë“±ìœ¼ë¡œ ë¬¶ì–´ì„œ ëª¨ë¸ ì…ë ¥ íŠ¹ì„±ìœ¼ë¡œ í™œìš©.


ì´ë ‡ê²Œ í•˜ë©´ ëª¨ë¸ì´ â€œTrainì—ì„œ ë³¸ ì  ì—†ëŠ” uid(ì‹ ê·œ ì¹´ë“œ)â€ë¼ë„, ì§‘ê³„ íŠ¹ì§•ì„ í†µí•´ ì‚¬ê¸° ì—¬ë¶€ë¥¼ ì¼ë°˜í™”í•´ì„œ íŒë‹¨í•  ìˆ˜ ìˆê²Œ ë¨.


ê°ì¢… ì „ì²˜ë¦¬, Feature Selection, EDA, Validation ì „ëµì„ ì´ë™ì›


430+ ì»¬ëŸ¼(V ì»¬ëŸ¼ë§Œ 339ê°œ)ì„ ë‹¤ë£¨ê¸° ìœ„í•´ PCA, ìƒê´€ê´€ê³„ ì œê±°, permutation importance, adversarial validation, time consistency check ë“±ì„ ìˆ˜í–‰.


ìµœì¢…ì ìœ¼ë¡œ CatBoost, LGBM, XGB ëª¨ë¸ì„ ê°ê° ë§Œë“¤ê³ , ì•™ìƒë¸”(+Stacking)ë¡œ LB ì„±ëŠ¥ì„ ë†’ì„.



- ì£¼ìš” ë‚´ìš© ì„¸ë¶€ ìš”ì•½


1) ì‹œê°„(Time)ì´ ì•„ë‹Œ ìƒˆë¡œìš´ í´ë¼ì´ì–¸íŠ¸ê°€ ê´€ê±´


ë°ì´í„°ê°€ TransactionDT ë“± ì‹œê°„ì¶•ì„ ê°€ì§€ì§€ë§Œ, ì‹¤ì œ ì¤‘ìš”í•œ ê±´ â€œTrainê³¼ Testê°€ ë‹¤ë¥¸ í´ë¼ì´ì–¸íŠ¸ë¥¼ ë§ì´ í¬í•¨â€í•œë‹¤ëŠ” ì .


Trainì—ì„œ ë³¸ í´ë¼ì´ì–¸íŠ¸(ì¹´ë“œ)ì™€ Testì— ìƒˆë¡œ ë“±ì¥í•œ í´ë¼ì´ì–¸íŠ¸(ì¹´ë“œ)ë¥¼ ì˜ êµ¬ë¶„í•´ì•¼ í•˜ê³ , ìƒˆë¡œ ë“±ì¥í•œ í´ë¼ì´ì–¸íŠ¸ì— ëŒ€í•´ ì¼ë°˜í™” ëŠ¥ë ¥ì„ ê°–ì¶°ì•¼ ë†’ì€ Private LB ì ìˆ˜ë¥¼ ì–»ìŒ.


2) ì‚¬ê¸° ê±°ë˜ = ì‚¬ê¸° í´ë¼ì´ì–¸íŠ¸


ì‚¬ê¸° ë°œìƒ í›„ ê·¸ ì¹´ë“œì˜ ëª¨ë“  ê±°ë˜ê°€ isFraud=1.


ì‹¤ì œë¡œ Train ë°ì´í„°ì—ì„œ ëŒ€ë¶€ë¶„ì˜ ì¹´ë“œëŠ” â€˜ëª¨ë‘ 0â€™ì´ê±°ë‚˜ â€˜ëª¨ë‘ 1â€™ì¸ í˜•íƒœ(0.2%ë§Œ 0,1 ì„ì—¬ ìˆìŒ).


ê²°êµ­ â€œì–´ëŠ ì‹ ìš©ì¹´ë“œê°€ ì‚¬ê¸°ì¸ì§€â€ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë¬¸ì œì™€ ìœ ì‚¬.


3) UID (Unique ID) ë°œìƒ


card1, addr1, ê·¸ë¦¬ê³  D1n(ì‹¤ì œ ê±°ë˜ ë‚ ì§œ day - D1ê°’) ë“± íŠ¹ì • ì»¬ëŸ¼ì„ ì¡°í•©í•´ í•˜ë‚˜ì˜ â€œuidâ€ë¥¼ ë§Œë“¤ë©´, í•´ë‹¹ uidëŠ” ê°™ì€ ì‹ ìš©ì¹´ë“œë¥¼ ì˜ë¯¸í•  ê°€ëŠ¥ì„±ì´ ë†’ìŒ.


ì£¼ì˜: Trainì— ì—†ì—ˆë˜ uidê°€ Testì— ë‚˜ì˜¤ë©´, ë°”ë¡œ â€œìƒˆë¡œìš´ ì¹´ë“œâ€ê°€ ë¨. ê·¸ë˜ì„œ uid ìì²´ë¥¼ ëª¨ë¸ì— ì§ì ‘ í”¼ì²˜ë¡œ ë„£ëŠ” ê±´ ìœ„í—˜(ê³¼ì í•© ê°€ëŠ¥, 68.2%ëŠ” Trainì— ì—†ëŠ” uid).


ëŒ€ì‹  uid ê·¸ë£¹ë³„ ì§‘ê³„(aggregate) í†µê³„ëŸ‰(mean, std, nunique ë“±)ì„ ëª¨ë¸ì— ì œê³µ â†’ ëª¨ë¸ì´ â€œì´ uidê°€ ê°€ì§„ ê±°ë˜ íŒ¨í„´â€ì„ í•™ìŠµ ê°€ëŠ¥.


4) ëª¨ë¸ë§ & í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§


CatBoost, LGBM, XGB ë“± ì—¬ëŸ¬ íŠ¸ë¦¬ ëª¨ë¸ ì‚¬ìš©


ì„œë¡œ ë‹¤ë¥¸ êµ¬í˜„ì²´/í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ â†’ ì•™ìƒë¸” ì‹œ ì„±ëŠ¥ì´ ìƒìŠ¹.


NN(Neural Network)ë„ ì‹œë„í–ˆì§€ë§Œ ìµœì¢… ì œì¶œì—ì„  ì œì™¸(ë¦¬ë”ë³´ë“œ ìƒìŠ¹í­ì´ ì‘ì•˜ë‹¤ê³  í•¨).


V ì»¬ëŸ¼(339ê°œ) ì²˜ë¦¬:
NAN êµ¬ì¡°ë‚˜ ìƒê´€ê´€ê³„ ë¬¶ìŒìœ¼ë¡œ ê·¸ë£¹í™” â†’ PCA / Averaging / Subset ë“±ìœ¼ë¡œ ì¶•ì†Œ.


ì–´ë–¤ V ë¸”ë¡ì€ ì‹œê°„ì— ë”°ë¼ íŒ¨í„´ì´ ë‹¬ë¼(â€œtime consistencyâ€ ê¹¨ì§) ì œê±°.


Feature Selection ê¸°ë²• ì´ë™ì›


Forward selection, Permutation importance, Adversarial validation, Correlation, Time consistency, Client consistency, train/test distribution ë“±.


â€œTime consistencyâ€ = ì´ˆë°˜ ë°ì´í„°ë¡œ í•™ìŠµí•´ ë’¤ìª½ (ë¯¸ë˜) ë°ì´í„° ì˜ˆì¸¡ â†’ AUCê°€ ê¸‰ê²©íˆ ë–¨ì–´ì§€ëŠ” í”¼ì²˜ëŠ” ë¯¸ë˜ ì¼ë°˜í™”ì— ë¶ˆë¦¬í•˜ë¯€ë¡œ ì œê±°.


í´ë¼ì´ì–¸íŠ¸ ì‹ë³„Adversarial validation(Train vs Test ë¶„ë¥˜)ì„ í†µí•´ ì–´ë–¤ ì»¬ëŸ¼ë“¤ì´ client ì‹ë³„ì— ì¤‘ìš”í•œì§€ ì ê²€ â†’ card1, addr1, C13, D1, D4, dist1, TransactionAmt ë“±ì´ ì¤‘ìš”í•˜ê²Œ ë‚˜íƒ€ë‚¨.


ì´ë¥¼ í™œìš©í•´ UID + Aggregation íŒŒìƒ í”¼ì²˜ ìƒì„±.


5) Validation ì „ëµ

   
í•œ ê°€ì§€ ë°©ì‹ë§Œ ë¯¿ì§€ ì•Šê³ , ì—¬ëŸ¬ ê°€ì§€ ì‹œë„:
ì²« 4ê°œì›” Train, ë§ˆì§€ë§‰ 1ê°œì›” Test


2ê°œì›” Train, 2ê°œì›” ê±´ë„ˆë›°ê³  2ê°œì›” Test ë“± ì‹œê³„ì—´ validation


LB ìì²´ë„ í•˜ë‚˜ì˜ validation ê°„ì£¼


GroupKFold by month


â€œSeen vs. Unseen clientâ€ separately í‰ê°€(UIDë¡œ ë¶„ë¥˜).


ëª¨ë¸ ê°„ ì•™ìƒë¸”ë¡œ â€œì´ë¯¸ ë³¸ í´ë¼ì´ì–¸íŠ¸ ì˜ˆì¸¡â€ê³¼ â€œìƒˆ í´ë¼ì´ì–¸íŠ¸ ì˜ˆì¸¡â€ì„ ëª¨ë‘ ë³´ì™„.


6) ìµœì¢… ì•™ìƒë¸” & Post Processing


CatBoost (0.9639/0.9408), LGBM (0.9617/0.9384), XGB (0.9602/0.9324) Public/Private LB ì„±ëŠ¥ì„ ê°ê° ë³´ì„.


ìµœì¢… ì œì¶œ ì‹œì—ëŠ” ë™ë“± ê°€ì¤‘ í‰ê·  or ìŠ¤íƒ ëª¨ë¸ â†’ Private LBë¥¼ ì•½ê°„ ë” ê°œì„ .


Post Processing(â€œPPâ€): ê°™ì€ uid(ê°™ì€ ì¹´ë“œ) ë‚´ ì˜ˆì¸¡ê°’ì„ í‰ê· /ê³ ì •(â€œëª¨ë“  ê±°ë˜ê°€ ë™ì¼ í™•ë¥ â€) ì²˜ë¦¬ â†’ LB ì†Œí­ ìƒìŠ¹(ì•½ +0.001).


# IEEE-CIS Fraud Detection í”„ë¡œì íŠ¸

## âœ¨ í”„ë¡œì íŠ¸ ê°œìš”

### ğŸ“… ì‚¬ìš© ë°ì´í„°: **IEEE-CIS Fraud Detection**

**ëª©í‘œ**: ê¸ˆìœµ ê±°ë˜ì—ì„œ ì‚¬ê¸° ì—¬ë¶€ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ê°œë°œ (Binary Classification)

**ì—­í•  :**

| ê¶Œìœ ì§„ | XGBoost + Cost-sensitive learning ì ìš©
(XGBoost + LGBM + catboost )+ Logistic Regression ì ìš©
XGBoost + Logistic Regression Stacking + Optuna + K-Fold (5-Fold) ì ìš© |
| --- | --- |
| ê¹€ì •ìˆ˜ | Xgboost
Xgboost + Random Forest
Xgboost + í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹  |
| ì£¼ì •ìš° | ëœë¤í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ êµ¬ì¶• ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹, ì‹ ê²½ë§ ì‹œí—˜ ì œì‘ |
| ì†¡ì„ ìœ  | RandomForest í™œìš© (SMOTE, Optuna)
RandomForest + XGBoost + LGBM 
RandomForest + XGBoost + Logistic Regression  |

### ğŸ”§ ë°ì´í„°ì…‹ ê°œìš”

- **ë°ì´í„° í¬ê¸°**: 59ë§Œ ê±´ (ì‚¬ê¸° ê±°ë˜ ë°ì´í„° 2ë§Œê±´ í¬í•¨)
- **ëª©í‘œ ë³€ìˆ˜**: `Is_Fraud` (0: ì •ìƒ ê±°ë˜, 1: ì‚¬ê¸° ê±°ë˜)
- **í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¬¸ì œ**: ì •ìƒ ê±°ë˜ (96.5%), ì‚¬ê¸° ê±°ë˜ (3.5%)

### **ğŸ“Œ ê¸ˆìœµ ê±°ë˜ ë°ì´í„° ë³€ìˆ˜ ì„¤ëª… í‘œ**

| **ì»¬ëŸ¼ëª…** | **ì„¤ëª…** | **ë¹„ê³ ** |
| --- | --- | --- |
| **TransactionDT** | ì£¼ì–´ì§„ ì°¸ì¡° ë‚ ì§œì‹œê°„ì˜ timedelta | ì‹¤ì œ íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ì•„ë‹˜ |
| **TransactionAMT** | ê±°ë˜ ì§€ë¶ˆ ê¸ˆì•¡ (USD) | ê±°ë˜ ê¸ˆì•¡ |
| **ProductCD** | ê° ê±°ë˜ì— ëŒ€í•œ ì œí’ˆ ì½”ë“œ | ì œí’ˆ ìœ í˜• êµ¬ë¶„ |
| **card1 - card6** | ì¹´ë“œ ìœ í˜•, ì¹´ë“œ ì¹´í…Œê³ ë¦¬, ë°œê¸‰ ì€í–‰, êµ­ê°€ ë“± ì§€ë¶ˆ ì¹´ë“œ ì •ë³´ | ì¹´ë“œ ì •ë³´ ê´€ë ¨ |
| **addr** | ì£¼ì†Œ ì •ë³´ | ìœ„ì¹˜ ê¸°ë°˜ íŠ¹ì„± |
| **dist** | ê±°ë¦¬ ì •ë³´ | êµ¬ë§¤ìì™€ íŒë§¤ì ê°„ ê±°ë¦¬ ë“± |
| **P_emaildomain** | êµ¬ë§¤ì ì´ë©”ì¼ ë„ë©”ì¸ | ì´ë©”ì¼ ê¸°ë°˜ íŠ¹ì§• |
| **R_emaildomain** | ìˆ˜ì‹ ì ì´ë©”ì¼ ë„ë©”ì¸ | ì´ë©”ì¼ ê¸°ë°˜ íŠ¹ì§• |
| **C1 - C14** | ê²°ì œ ì¹´ë“œì™€ ì—°ê´€ëœ ì£¼ì†Œì˜ ìˆ˜ ë“± ê³„ì‚°ëœ ê°’ | ì‹¤ì œ ì˜ë¯¸ëŠ” ê°€ë ¤ì ¸ ìˆìŒ |
| **D1 - D15** | ì´ì „ ê±°ë˜ ì‚¬ì´ì˜ ë‚ ì§œ ë“± íƒ€ì„ë¸íƒ€ | ì‹œê°„ ê°„ê²© ê´€ë ¨ |
| **M1 - M9** | ì¼ì¹˜ ì—¬ë¶€ (ì˜ˆ: ì¹´ë“œì— ìˆëŠ” ì´ë¦„ê³¼ ì£¼ì†Œê°€ ì¼ì¹˜í•˜ëŠ”ì§€ ë“±) | ì¸ì¦ ë° ê²€ì¦ ì •ë³´ |
| **Vxxx** | Vestaì—ì„œ ì„¤ê³„í•œ ìˆœìœ„, ê³„ì‚° ë° ê¸°íƒ€ ì—”í„°í‹° ê´€ê³„ ê¸°ë°˜ ê¸°ëŠ¥ | ìƒì„¸í•œ ì˜ë¯¸ëŠ” ë¹„ê³µê°œ |

ë°ì´í„° ì„¤ëª… ë§í¬: [IEEE-CIS Fraud Detection | Kaggle](https://www.kaggle.com/competitions/ieee-fraud-detection/discussion/101203)

ğŸ’¡ **ğŸ‘‰ ì£¼ìš” íŠ¹ì§•:**

- **ì‹œê°„(Timestamp) ê´€ë ¨ ì •ë³´** â†’ `TransactionDT`, `D1-D15`
- **ê±°ë˜ ê¸ˆì•¡ & ì œí’ˆ ì½”ë“œ** â†’ `TransactionAMT`, `ProductCD`
- **ì¹´ë“œ ì •ë³´** â†’ `card1-card6`
- **ìœ„ì¹˜ ê¸°ë°˜ ì •ë³´** â†’ `addr`, `dist`
- **ì´ë©”ì¼ ê´€ë ¨ ì •ë³´** â†’ `P_emaildomain`, `R_emaildomain`
- **ì£¼ì†Œ ë° ì¹´ë“œ ì—°ê´€ ë³€ìˆ˜** â†’ `C1-C14`, `M1-M9`
- **ë¹„ê³µê°œ ì—”í„°í‹° ê³„ì‚° ë³€ìˆ˜** â†’ `Vxxx`

ğŸš€ **ì´ ë°ì´í„°ëŠ” ê¸ˆìœµ ê±°ë˜ íŒ¨í„´ ë¶„ì„ ë° ì‚¬ê¸° íƒì§€ë¥¼ ìœ„í•´ ë‹¤ì–‘í•œ ë³€ìˆ˜ë“¤ë¡œ êµ¬ì„±ë¨!**

## ğŸš€ ëª¨ë¸ ì„ íƒ: ë¨¸ì‹ ëŸ¬ë‹ vs ë”¥ëŸ¬ë‹

### âœ… ë¨¸ì‹ ëŸ¬ë‹ì´ ì í•©í•œ ì´ìœ 

- **í‘œ í˜•ì‹ ë°ì´í„° (Tabular Data)** â†’ íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸ì´ íš¨ê³¼ì 
- **ë°ì´í„° í¬ê¸° (59ë§Œ ê±´)** â†’ ë¨¸ì‹ ëŸ¬ë‹ë§Œìœ¼ë¡œë„ ì¶©ë¶„í•œ ì„±ëŠ¥ í™•ë³´ ê°€ëŠ¥
- **ë”¥ëŸ¬ë‹ì€ ë¶ˆí•„ìš”í•œ ë³µì¡ì„±ì„ ìœ ë°œ** (í•™ìŠµ ì‹œê°„ ì¦ê°€, í•´ì„ ì–´ë ¤ì›€)
- **ëœë¤ í¬ë ˆìŠ¤íŠ¸, XGBoost ê°™ì€ íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸ì´ ê¸ˆìœµê¶Œì—ì„œë„ ê²€ì¦ëœ ë°©ë²•**
- ì•½ 600ê°œ íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§„ ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ êµ¬í˜„í•´ ë³´ì•˜ì§€ë§Œ, ëª¨ë“  ë°ì´í„°ë¥¼ ì •ìƒ ê±°ë˜ ë°ì´í„°ë¡œ íŒë‹¨í•¨.

### ë”¥ëŸ¬ë‹ êµ¬í˜„ ì½”ë“œ

```python
class ANNModel(nn.Module):
    def __init__(self, input_size=225):
        super(ANNModel, self).__init__()
        self.layer = nn.Sequential(
            nn.Linear(input_size, 500),
            nn.ReLU(),
            nn.Linear(500, 200),
            nn.ReLU(),
            nn.Linear(200, 100),
            nn.ReLU(),
            nn.Linear(100, 20),
            nn.ReLU(),
            nn.Linear(20, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        x = self.layer(x)
        return x

device = "cuda" if torch.cuda.is_available() else "cpu"
model = ANNModel().to(device)
model.train()
criterion = nn.BCELoss().to(device)
optimizer = optim.Adam(model.parameters(), lr=0.1)

for epoch in range(100):
    cost = 0.0

    for x, y in train_loader:
        x = x.to(device)
        y = y.to(device)

        output = model(x)
        loss = criterion(output, y.unsqueeze(1))

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        cost += loss

    cost = cost / len(train_loader)

    if (epoch + 1) % 10 == 0:
        print(f"Epoch : {epoch+1:4d}, Model : {list(model.parameters())}, Cost : {cost:.3f}")
```

í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ í‰ê°€ ê²°ê³¼ (í˜¼ë™ í–‰ë ¬):

![image.png](attachment:1ba2044f-1e95-46a9-9aa9-d0f3e4a97a11:image.png)

## ğŸ’¡ ëª¨ë¸ ì„ ì • ë° í‰ê°€

### ğŸŒŸ ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸: **ëœë¤ í¬ë ˆìŠ¤íŠ¸**

- ë³€ìˆ˜ ê°„ ë¹„ì„ í˜• ê´€ê³„ë¥¼ ì˜ íƒì§€í•˜ê³ , ë¶ˆê· í˜• ë°ì´í„°ì— ê°•í•¨

### **ğŸ“Š ëª¨ë¸ í‰ê°€ ì§€í‘œ (Classification Report)**

| **Metric** | **Non-Fraud** | **Fraud** | **Macro Avg** |
| --- | --- | --- | --- |
| **Precision** | **0.98** | 0.95 | **0.96** |
| **Recall** | **1.00** | 0.41 | **0.70** |
| **F1-Score** | **0.99** | 0.57 | **0.78** |
| **Support** | 170,821 | 6,341 | 177,162 |
- **Accuracy**: 98%
- **Weighted Avg F1-Score**: 97%

### ğŸ”§ ëª¨ë¸ ê°œì„  ê³¼ì •

### **0. ë°ì´í„° ì²˜ë¦¬**

âœ… **ê²°ì¸¡ì¹˜ ì²˜ë¦¬**

- `fillna(-999)`ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  ê²°ì¸¡ì¹˜ë¥¼ -999ë¡œ ì±„ì›€ (XGBoostëŠ” ê²°ì¸¡ì¹˜ ìì²´ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìŒ)

âœ… **ë¼ë²¨ ì¸ì½”ë”©(Label Encoding)**

- ë²”ì£¼í˜• ë³€ìˆ˜(object íƒ€ì…)ë¥¼ LabelEncoderë¥¼ ì‚¬ìš©í•˜ì—¬ ìˆ«ìë¡œ ë³€í™˜
- í›ˆë ¨ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ë²”ì£¼ë¥¼ ë™ì¼í•˜ê²Œ ë§ì¶”ê¸° ìœ„í•´ ëª¨ë“  í´ë˜ìŠ¤ ì •ë³´ë¥¼ ì €ì¥ í›„ ë³€í™˜

âœ… **í›ˆë ¨ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„° ë¶„í• **

- `train_test_split()`ì„ ì‚¬ìš©í•˜ì—¬ **80%** ë°ì´í„°ë¥¼ í›ˆë ¨ ë°ì´í„°ë¡œ, **20%** ë°ì´í„°ë¥¼ ê²€ì¦ ë°ì´í„°ë¡œ ë¶„ë¦¬

### 1. **XGBoost ì ìš©**

- **ëœë¤ í¬ë ˆìŠ¤íŠ¸ë³´ë‹¤ ë†’ì€ ì˜ˆì¸¡ ì„±ëŠ¥ê³¼ ë¹ ë¥¸ í•™ìŠµ ì†ë„**
- **ì‚¬ê¸° íƒì§€ì—ì„œ ê°€ì¥ ë†’ì€ Precision & Recall ê¸°ë¡**
- í•˜ì§€ë§Œ **ì¬í˜„ìœ¨(Recall)ì´ ë‚®ìŒ** â†’ ì¶”ê°€ ê°œì„  í•„ìš”

![image.png](attachment:4ae9ee90-2eb2-449b-a74b-f5901f1b26c0:image.png)

![image.png](attachment:2e0aa346-c61e-45b1-bab4-70eebb958e97:image.png)

- **XGBoost ì½”ë“œ**
    
    ```python
    import gc
    import numpy as np
    import pandas as pd
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import LabelEncoder
    from sklearn.metrics import confusion_matrix, classification_report
    from sklearn.metrics import precision_recall_curve
    import xgboost as xgb
    import seaborn as sns
    import matplotlib.pyplot as plt
    #############################
    # 1) ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
    #############################
    train_transaction = pd.read_csv('/content/train_transaction.csv', index_col='TransactionID')
    train_identity    = pd.read_csv('/content/train_identity.csv',    index_col='TransactionID')
    test_transaction  = pd.read_csv('/content/test_transaction.csv',  index_col='TransactionID')
    test_identity     = pd.read_csv('/content/test_identity.csv',     index_col='TransactionID')
    sample_submission = pd.read_csv('/content/sample_submission.csv', index_col='TransactionID')
    print("train_transaction:", train_transaction.shape)
    print("train_identity:", train_identity.shape)
    print("test_transaction:", test_transaction.shape)
    print("test_identity:", test_identity.shape)
    #############################
    # 2) ì»¬ëŸ¼ëª… ì¹˜í™˜ ( '-' â†’ '_' )
    #############################
    train_transaction.columns = [c.replace('-', '_') for c in train_transaction.columns]
    train_identity.columns    = [c.replace('-', '_') for c in train_identity.columns]
    test_transaction.columns  = [c.replace('-', '_') for c in test_transaction.columns]
    test_identity.columns     = [c.replace('-', '_') for c in test_identity.columns]
    #############################
    # 3) Train ë³‘í•©, Test ë³‘í•©
    #############################
    train = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)
    test  = test_transaction.merge(test_identity,   how='left', left_index=True, right_index=True)
    del train_transaction, train_identity, test_transaction, test_identity
    gc.collect()
    print("Train shape (merged):", train.shape)
    print("Test  shape (merged):", test.shape)
    #############################
    # 4) íƒ€ê¹ƒ ë¶„ë¦¬
    #############################
    y_all = train['isFraud'].copy()
    X_all = train.drop('isFraud', axis=1)
    # í…ŒìŠ¤íŠ¸ ì„¸íŠ¸
    X_test = test.copy()
    del train, test
    gc.collect()
    #############################
    # 5) ê°„ë‹¨í•œ ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    #############################
    X_all  = X_all.fillna(-999)
    X_test = X_test.fillna(-999)
    #############################
    # 6) ë¼ë²¨ ì¸ì½”ë”©
    #############################
    for col in X_all.columns:
        if X_all[col].dtype == 'object' or X_test[col].dtype == 'object':
            le = LabelEncoder()
            le.fit(list(X_all[col].values) + list(X_test[col].values))
            X_all[col]  = le.transform(X_all[col].values)
            X_test[col] = le.transform(X_test[col].values)
    print("X_all shape:", X_all.shape)
    print("X_test shape:", X_test.shape)
    print("y_all shape:", y_all.shape)
    #############################
    # 7) Train/Validation ë¶„í• 
    #############################
    X_train, X_val, y_train, y_val = train_test_split(
        X_all, y_all,
        test_size=0.2,
        random_state=42,
        stratify=y_all
    )
    print("X_train:", X_train.shape, "X_val:", X_val.shape)
    #############################
    # 8) ëª¨ë¸ í•™ìŠµ (ê²€ì¦ ì§€í‘œ í™•ì¸ìš©)
    #############################
    clf = xgb.XGBClassifier(
        n_estimators=250,
        max_depth=9,
        learning_rate=0.05,
        subsample=0.8,
        colsample_bytree=0.4,
        missing=-999,
        random_state=42,
        # XGBoost 2.0+ì—ì„œ GPU ì‚¬ìš© ì‹œ
        tree_method='hist',
        device='cuda'
    )
    clf.fit(X_train, y_train)
    #############################
    # 9) ê²€ì¦ ì„¸íŠ¸ ì˜ˆì¸¡ â†’ í˜¼ë™í–‰ë ¬, ì •ë°€ë„/ì¬í˜„ìœ¨, PR ì»¤ë¸Œ
    #############################
    # (1) 0/1 ì˜ˆì¸¡
    y_val_pred = clf.predict(X_val)
    # (2) í˜¼ë™í–‰ë ¬
    cm = confusion_matrix(y_val, y_val_pred)
    print("\n=== Confusion Matrix ===")
    print(cm)
    plt.figure(figsize=(5,4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title("Confusion Matrix (Validation)")
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.show()
    # (3) ì •ë°€ë„/ì¬í˜„ìœ¨ ë¦¬í¬íŠ¸
    print("\n=== Classification Report ===")
    print(classification_report(y_val, y_val_pred, digits=4))
    # (4) Precision-Recall Curve (ê²€ì¦ ì„¸íŠ¸ í™•ë¥  ì˜ˆì¸¡ ì‚¬ìš©)
    y_val_proba = clf.predict_proba(X_val)[:, 1]  # ì‚¬ê¸°(1) í™•ë¥ 
    precision, recall, thresholds = precision_recall_curve(y_val, y_val_proba)
    plt.figure(figsize=(6,5))
    plt.plot(recall, precision, color='darkorange', lw=2)
    plt.title("Precision-Recall Curve (Validation)")
    plt.xlabel("Recall")
    plt.ylabel("Precision")
    plt.grid(True)
    plt.show()
    #############################
    # 10) ìµœì¢… ì „ì²´ í•™ìŠµ & ì œì¶œ íŒŒì¼ ìƒì„±
    #############################
    # (ì„ íƒ) ë§Œì•½ ê²€ì¦ ì ìˆ˜ê°€ ê´œì°®ë‹¤ë©´, ì „ì²´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ ì¬í•™ìŠµ
    final_model = xgb.XGBClassifier(
        n_estimators=250,
        max_depth=9,
        learning_rate=0.05,
        subsample=0.8,
        colsample_bytree=0.4,
        missing=-999,
        random_state=42,
        tree_method='hist',
        device='cuda'
    )
    final_model.fit(X_all, y_all)
    # í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ì˜ˆì¸¡
    y_test_proba = final_model.predict_proba(X_test)[:, 1]
    sample_submission['isFraud'] = y_test_proba
    sample_submission.to_csv('simple_xgboost_plt.csv')
    print("Done! Submission file saved: simple_xgboost_plt.csv")
    ```
    

### 2. **XGBoost + Logistic Regression Stacking + Optuna + K-Fold (5-Fold) ì ìš©**

- âœ… **XGBoostì˜ ì˜ˆì¸¡ í™•ë¥ ê°’ì„ Logistic Regressionì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©**
    - XGBoost ë‹¨ì¼ ëª¨ë¸ì˜ ê°•ë ¥í•œ ì˜ˆì¸¡ë ¥ì„ í™œìš©í•˜ë©´ì„œ, ë¡œì§€ìŠ¤í‹± íšŒê·€ë¡œ ì„±ëŠ¥ í–¥ìƒ
- âœ… **Optunaë¥¼ í™œìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”** (n_estimators, learning_rate, max_depth ë“±ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ XGBoost í•™ìŠµ ì„±ëŠ¥ ê·¹ëŒ€í™”
    - ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ XGBoost í•™ìŠµ ì„±ëŠ¥ ê·¹ëŒ€í™”
- âœ… **K-Fold(5-Fold) Cross Validationì„ ì ìš©í•˜ì—¬ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ**
    - ë°ì´í„°ë¥¼ 5ê°œ Foldë¡œ ë‚˜ëˆ„ì–´ XGBoost ëª¨ë¸ì´ ë” ì˜ í•™ìŠµí•˜ë„ë¡ í•¨

![image.png](attachment:81c0f421-3bd5-49dd-9a4b-eae5aed46b5b:image.png)

![image.png](attachment:e35b5e35-5476-4430-981a-945d0970f0cb:image.png)

![image.png](attachment:e19d32bc-f416-4dce-8bee-1d4c5c1d2708:image.png)

- **XGBoost + Logistic Regression Stacking + Optuna + K-Fold (5-Fold) ì½”ë“œ**
    
    ### **XGBoost í•™ìŠµ (Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”)**
    
    âœ… **Optunaë¥¼ ì‚¬ìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹**
    
    - `objective()` í•¨ìˆ˜ ë‚´ì—ì„œ Optunaë¥¼ í™œìš©í•´ **ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°**ë¥¼ íƒìƒ‰
    - í•™ìŠµ ì†ë„(`learning_rate`), íŠ¸ë¦¬ ê¹Šì´(`max_depth`), ë¶€íŠ¸ìŠ¤íŠ¸ë©(`subsample`), ì •ê·œí™”(`reg_lambda`, `reg_alpha`) ë“±ì„ ì¡°ì •
    - 5-Fold Cross Validationì„ ìˆ˜í–‰í•˜ì—¬ ìµœì ì˜ ëª¨ë¸ì„ ì°¾ìŒ
    
    âœ… **ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ì€ í›„ ëª¨ë¸ í•™ìŠµ**
    
    - `best_params_xgb`ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì ì˜ XGBoost ëª¨ë¸ì„ ìƒì„±í•˜ê³ , í›ˆë ¨ ë°ì´í„°ì—ì„œ í•™ìŠµ ì§„í–‰
    
    ---
    
    ### **K-Fold Cross Validationì„ í™œìš©í•œ XGBoost ì˜ˆì¸¡**
    
    âœ… **5-Fold Cross Validationì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ ì§„í–‰**
    
    - `StratifiedKFold(n_splits=5, shuffle=True, random_state=42)`ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì…‹ì„ 5ê°œì˜ Foldë¡œ ë‚˜ëˆ”
    - XGBoost ëª¨ë¸ì„ **ê° Foldì— ëŒ€í•´ í•™ìŠµ í›„ ê²€ì¦ ë°ì´í„°ë¥¼ ì˜ˆì¸¡**
    - ê° Foldì—ì„œ ë‚˜ì˜¨ ì˜ˆì¸¡ í™•ë¥ ì„ ì €ì¥í•˜ê³ , í‰ê· ì„ ë‚´ì–´ ìµœì¢…ì ì¸ í™•ë¥ ê°’ì„ ìƒì„±
    
    âœ… **XGBoost ì˜ˆì¸¡ê°’ ì €ì¥**
    
    - `xgb_train_meta[val_idx] = clf.predict_proba(X_v)[:, 1]` â†’ ê²€ì¦ ë°ì´í„° í™•ë¥ ê°’ ì €ì¥
    - `xgb_val_meta += clf.predict_proba(X_val)[:, 1] / kf.n_splits` â†’ ê²€ì¦ ë°ì´í„° ì˜ˆì¸¡ê°’ ì €ì¥
    - `xgb_test_meta += clf.predict_proba(X_test)[:, 1] / kf.n_splits` â†’ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ê°’ ì €ì¥
    
    ---
    
    ### **Logistic Regression Stacking**
    
    âœ… **Logistic Regressionì„ í™œìš©í•œ Stacking**
    
    - XGBoostì—ì„œ ë‚˜ì˜¨ **ê²€ì¦ ë°ì´í„°ì˜ ì˜ˆì¸¡ í™•ë¥ ê°’ì„ Logistic Regressionì˜ ì…ë ¥ê°’ìœ¼ë¡œ ì‚¬ìš©**
    - Logistic Regressionì€ **XGBoostë³´ë‹¤ í•´ì„ ê°€ëŠ¥ì„±ì´ ë†’ê³ , ê³¼ì í•©ì„ ì¤„ì´ëŠ” íš¨ê³¼**ê°€ ìˆìŒ
    - `log_reg.fit(X_train_meta, y_train)` â†’ XGBoostì˜ ì˜ˆì¸¡ê°’ì„ Logistic Regression ëª¨ë¸ì˜ í›ˆë ¨ ë°ì´í„°ë¡œ í™œìš©
    
    âœ… **ê²€ì¦ ë°ì´í„°ì—ì„œ ìµœì¢… ì˜ˆì¸¡ ìˆ˜í–‰**
    
    - `y_val_pred_logreg = log_reg.predict(X_val_meta)`
    - `confusion_matrix(y_val, y_val_pred_logreg)`ì„ ì‚¬ìš©í•˜ì—¬ ê²€ì¦ ë°ì´í„°ì˜ ì„±ëŠ¥ì„ í‰ê°€
